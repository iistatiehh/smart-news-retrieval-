{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ed9fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\miniconda3\\envs\\NewsRetrival\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %pip install elasticsearch sentence-transformers\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d85db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"news_reuters_docs\"\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    request_timeout=60,\n",
    "    max_retries=3,\n",
    "    retry_on_timeout=True\n",
    ")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca68f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add this to clean up old indices\n",
    "print(\"Cleaning up...\")\n",
    "try:\n",
    "    # Delete old problematic indices\n",
    "    all_indices = es.cat.indices(format='json')\n",
    "    for idx in all_indices:\n",
    "        if idx['health'] == 'red' or idx['status'] == 'close':\n",
    "            print(f\"Deleting problematic index: {idx['index']}\")\n",
    "            es.indices.delete(index=idx['index'], ignore=[400, 404])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a004311",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping ={\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"max_ngram_diff\": 5\n",
    "    },\n",
    "    \"analysis\": {\n",
    "      \"char_filter\": {\n",
    "        \"html_strip\": { \"type\": \"html_strip\" }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"length_filter\": { \"type\": \"length\", \"min\": 3 }\n",
    "      },\n",
    "      \"tokenizer\": {\n",
    "        \"autocomplete_infix_tokenizer\": {\n",
    "          \"type\": \"ngram\",\n",
    "          \"min_gram\": 3,\n",
    "          \"max_gram\": 8,\n",
    "          \"token_chars\": [\"letter\", \"digit\"]\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"autocomplete_infix\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"autocomplete_infix_tokenizer\",\n",
    "          \"filter\": [\"lowercase\"]\n",
    "        },\n",
    "        \"autocomplete_infix_search\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"lowercase\"\n",
    "        },\n",
    "        \"content_analyzer\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"char_filter\": [\"html_strip\"],\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\", \"stop\", \"length_filter\", \"porter_stem\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"autocomplete_infix\",\n",
    "        \"search_analyzer\": \"autocomplete_infix_search\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"title_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 384,\n",
    "        \"index\": True,\n",
    "        \"similarity\": \"cosine\",\n",
    "        \"index_options\": { \"type\": \"hnsw\", \"m\": 16, \"ef_construction\": 100 }\n",
    "      },\n",
    "      \"content_chunks\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"text\": { \"type\": \"text\" },\n",
    "          \"vector\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 384,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"cosine\",\n",
    "            \"index_options\": { \"type\": \"hnsw\", \"m\": 16, \"ef_construction\": 100 }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"content\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"content_analyzer\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"authors\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"first_name\": { \"type\": \"text\" },\n",
    "          \"last_name\": { \"type\": \"text\" },\n",
    "          \"email\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"date\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"strict_date_optional_time||yyyy-MM-dd'T'HH:mm:ss||epoch_millis\"\n",
    "      },\n",
    "      \"dateline\": { \"type\": \"text\" },\n",
    "      \"geo_location\": { \"type\": \"geo_point\" },\n",
    "      \"temporalExpressions\": { \"type\": \"keyword\" },\n",
    "      \"georeferences\": { \"type\": \"keyword\" },\n",
    "      \"places\": { \"type\": \"keyword\" },\n",
    "      \"geopoints\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"place\": { \"type\": \"keyword\" },\n",
    "          \"location\": { \"type\": \"geo_point\" }\n",
    "        }\n",
    "      },\n",
    "      \"topics\": { \"type\": \"keyword\" },\n",
    "      \"people\": { \"type\": \"keyword\" },\n",
    "      \"orgs\": { \"type\": \"keyword\" },\n",
    "      \"exchanges\": { \"type\": \"keyword\" },\n",
    "      \"companies\": { \"type\": \"keyword\" }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e8479b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(es.ping())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07393056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for index to be ready...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'cluster_name': 'elasticsearch', 'status': 'yellow', 'timed_out': False, 'number_of_nodes': 1, 'number_of_data_nodes': 1, 'active_primary_shards': 45, 'active_shards': 45, 'relocating_shards': 0, 'initializing_shards': 0, 'unassigned_shards': 2, 'unassigned_primary_shards': 0, 'delayed_unassigned_shards': 0, 'number_of_pending_tasks': 0, 'number_of_in_flight_fetch': 0, 'task_max_waiting_in_queue_millis': 0, 'active_shards_percent_as_number': 95.74468085106383})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, body=mapping)\n",
    "\n",
    "# Add this - wait for index to be ready\n",
    "print(\"Waiting for index to be ready...\")\n",
    "time.sleep(5)\n",
    "es.cluster.health(wait_for_status='yellow', timeout='30s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e87fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 21578 documents\n",
      "Processing batch 1 (0 to 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_33044\\721800356.py:103: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  success, failed = helpers.bulk(es, actions, raise_on_error=False, request_timeout=120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 2 (500 to 1000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 3 (1000 to 1500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 4 (1500 to 2000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 5 (2000 to 2500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 6 (2500 to 3000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 7 (3000 to 3500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 8 (3500 to 4000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 9 (4000 to 4500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 10 (4500 to 5000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 11 (5000 to 5500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 12 (5500 to 6000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 13 (6000 to 6500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 14 (6500 to 7000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 15 (7000 to 7500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 16 (7500 to 8000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 17 (8000 to 8500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 18 (8500 to 9000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 19 (9000 to 9500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 20 (9500 to 10000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 21 (10000 to 10500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 22 (10500 to 11000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 23 (11000 to 11500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 24 (11500 to 12000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 25 (12000 to 12500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 26 (12500 to 13000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 27 (13000 to 13500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 28 (13500 to 14000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 29 (14000 to 14500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 30 (14500 to 15000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 31 (15000 to 15500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 32 (15500 to 16000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 33 (16000 to 16500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 34 (16500 to 17000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 35 (17000 to 17500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 36 (17500 to 18000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 37 (18000 to 18500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 38 (18500 to 19000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 39 (19000 to 19500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 40 (19500 to 20000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 41 (20000 to 20500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 42 (20500 to 21000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 43 (21000 to 21500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 44 (21500 to 21578)\n",
      "  Indexed 78 documents (Failed: 0)\n",
      "\n",
      "Total indexed: 21578 documents successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 1, 'failed': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, time, re\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\asus\\Desktop\\NewsRetrival\\smart-news-retrieval-\\output\\reuters_full.json\"\n",
    "\n",
    "def chunk_text(text, max_len=500):\n",
    "    \"\"\"Split text into chunks of max_len characters.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sent in sentences:\n",
    "        if len(current_chunk) + len(sent) <= max_len:\n",
    "            current_chunk += \" \" + sent if current_chunk else sent\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sent\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "print(\"Loading documents...\")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "batch_size = 500\n",
    "total_indexed = 0\n",
    "\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch = documents[i:i + batch_size]\n",
    "    actions = []\n",
    "\n",
    "    print(f\"Processing batch {i//batch_size + 1} ({i} to {min(i+batch_size, len(documents))})\")\n",
    "\n",
    "    for doc in batch:\n",
    "        title = doc.get(\"title\", \"\")\n",
    "        content = doc.get(\"content\", \"\")\n",
    "        date = doc.get(\"date\")\n",
    "        dateline = doc.get(\"dateline\", \"\")\n",
    "        places = doc.get(\"places\", [])\n",
    "        temporal = doc.get(\"temporalExpressions\", [])\n",
    "        georefs = doc.get(\"georeferences\", [])\n",
    "\n",
    "        authors = []\n",
    "        for author in doc.get(\"authors\", []):\n",
    "            if author:\n",
    "                authors.append({\n",
    "                    \"first_name\": author.get(\"first_name\", \"\"),\n",
    "                    \"last_name\": author.get(\"last_name\", \"\"),\n",
    "                    \"email\": author.get(\"email\", \"\")\n",
    "                })\n",
    "\n",
    "        geopoints = []\n",
    "        for g in doc.get(\"geopoints\", []):\n",
    "            if g and g.get(\"location\"):\n",
    "                loc = g[\"location\"]\n",
    "                lat = loc.get(\"lat\")\n",
    "                lon = loc.get(\"lon\")\n",
    "                place = g.get(\"place\", \"\")\n",
    "                \n",
    "                if lat is not None and lon is not None:\n",
    "                    geopoints.append({\n",
    "                        \"place\": place,\n",
    "                        \"location\": {\"lat\": lat, \"lon\": lon}\n",
    "                    })\n",
    "        \n",
    "        geo_location = None\n",
    "        if geopoints:\n",
    "            geo_location = geopoints[0][\"location\"]\n",
    "        \n",
    "        # title vector\n",
    "        title_vector = model.encode(title).tolist()\n",
    "\n",
    "        # chunk content and encode each chunk\n",
    "        content_chunks = chunk_text(content, max_len=500)\n",
    "        content_chunks_encoded = [\n",
    "            {\"text\": chunk, \"vector\": model.encode(chunk).tolist()} for chunk in content_chunks\n",
    "        ]\n",
    "\n",
    "        es_doc = {\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_source\": {\n",
    "                \"title\": title,\n",
    "                \"title_vector\": title_vector,\n",
    "                \"content\": content,\n",
    "                \"content_chunks\": content_chunks_encoded,\n",
    "                \"authors\": authors,\n",
    "                \"date\": date,\n",
    "                \"dateline\": dateline,\n",
    "                \"places\": places,\n",
    "                \"temporalExpressions\": temporal,\n",
    "                \"georeferences\": georefs,\n",
    "                \"geopoints\": geopoints,\n",
    "                \"geo_location\": geo_location\n",
    "            }\n",
    "        }\n",
    "        actions.append(es_doc)\n",
    "\n",
    "    try:\n",
    "        success, failed = helpers.bulk(es, actions, raise_on_error=False, request_timeout=120)\n",
    "        total_indexed += success\n",
    "        print(f\"  Indexed {success} documents (Failed: {len(failed)})\")\n",
    "        if failed:\n",
    "            print(f\"  First error: {failed[0]}\")\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error in batch: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nTotal indexed: {total_indexed} documents successfully.\")\n",
    "es.indices.refresh(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceafddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test 1: Basic search ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_12944\\3695856247.py:101: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  response = es.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 results:\n",
      "1. KUMAGAI GUMI'S UNIT SEEKS LISTING IN HONG KONG (score: 3.76)\n",
      "2. ECONOMIC SPOTLIGHT - U.S. BANKING REGULATION (score: 2.46)\n",
      "3. BANKERS PREDICT SHAKEOUT IN SWEDISH BANKING SYSTEM (score: 1.59)\n",
      "4. ECONOMIC SPOTLIGHT - BANKAMERICA <BAC> (score: 1.59)\n",
      "5. FIRST REPUBLICBANK <FRB> TROUBLED LOANS RISE (score: 1.57)\n",
      "6. ECONOMIC SPOTLIGHT - FOREIGN BANKS IN GERMANY (score: 1.56)\n",
      "7. DANISH PLAN TO HELP CRISIS-HIT BANK DEPOSITORS (score: 1.56)\n",
      "8. BANKAMERICA <BAC> CHAIR SAYS BANK TURNING AROUND (score: 1.56)\n",
      "9. GERMAN BANKS FACE DISRUPTION AFTER TALKS COLLAPSE (score: 1.55)\n",
      "10. SAVINGS BANK CLOSED, 15TH TROUBLED U.S. S&L OF YEAR (score: 1.55)\n",
      "\n",
      "=== Test 2: Fuzzy search (with typo) ===\n",
      "Found 10 results with typos corrected:\n",
      "1. KUMAGAI GUMI'S UNIT SEEKS LISTING IN HONG KONG\n",
      "2. BANK OF FRANCE SELLS 1.6 BILLION FRANCS CRH TAP\n",
      "3. SOUTHOLD SAVINGS BANK <SDSB.O> 3RD QTR NET\n",
      "\n",
      "=== Test 3: Location-aware search ===\n",
      "Found 10 results near London:\n",
      "1. BANKING CENTER <TBCX> 1ST QTR NET\n",
      "   Location: None\n",
      "2. BANKING CENTER <TBCX.O> 3RD QTR NET\n",
      "   Location: None\n",
      "3. SIGNET BANKING CORP <SBK> 3RD QTR NET\n",
      "   Location: None\n",
      "\n",
      "=== Test 4: Temporal search (1987 documents) ===\n",
      "Found 10 results from 1987:\n",
      "1. ECONOMIC SPOTLIGHT - U.S. BANKING REGULATION\n",
      "   Date: 1987-03-17T08:57:16\n",
      "2. WORTHERN BANKING <WOR> IN AGREEMENT WITH FEDS\n",
      "   Date: 1987-03-25T14:36:52\n",
      "3. ECONOMIC SPOTLIGHT - FOREIGN BANKS IN GERMANY\n",
      "   Date: 1987-04-07T08:25:50\n",
      "\n",
      "=== Test 5: Combined spatio-temporal query ===\n",
      "Found 5 German news near Hamburg in March 1987:\n",
      "1. <GERMANTOWN SAVINGS BANK> IN SUBSCRIPTION OFFER\n",
      "   Date: 1987-03-19T13:06:41\n",
      "   Places: usa\n",
      "\n",
      "2. BERTELSMANN TO MARKET APPLE SOFTWARE IN GERMANY\n",
      "   Date: 1987-03-09T11:58:53\n",
      "   Places: west-germany\n",
      "\n",
      "3. EC INFLATION STARTS TO RISE AGAIN IN FEBRUARY\n",
      "   Date: 1987-03-25T12:49:54\n",
      "   Places: luxembourg\n",
      "\n",
      "4. FRENCH INFLATION SLOWS IN FEBRUARY\n",
      "   Date: 1987-03-16T10:48:22\n",
      "   Places: france\n",
      "\n",
      "5. INFLATION STILL A CONCERN, VOLCKER SAYS\n",
      "   Date: 1987-03-12T16:48:07\n",
      "   Places: usa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search(query, user_location=None, temporal_filter=None, top_k=10):\n",
    "    \"\"\"\n",
    "    Enhanced hybrid search with fuzzy matching, recency boosting, and location-aware ranking.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        user_location: Dict with 'lat' and 'lon' for geo-proximity ranking (optional)\n",
    "        temporal_filter: Dict with 'start' and 'end' dates for temporal filtering (optional)\n",
    "        top_k: Number of results to return\n",
    "    \"\"\"\n",
    "    query_vector = model.encode(query).tolist()\n",
    "    \n",
    "    # Build the base query with function_score for recency and location boosting\n",
    "    query_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"function_score\": {\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\": [\n",
    "                            {\n",
    "                                \"multi_match\": {\n",
    "                                    \"query\": query,\n",
    "                                    \"fields\": [\"title^3\", \"content\"],\n",
    "                                    \"type\": \"best_fields\",\n",
    "                                    \"fuzziness\": \"AUTO\",  # Handles typos (1-2 char edits)\n",
    "                                    \"prefix_length\": 2,   # First 2 chars must match exactly\n",
    "                                    \"max_expansions\": 50  # Limit fuzzy term expansions\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"functions\": [\n",
    "                    # Recency boost - exponential decay favoring recent documents\n",
    "                    {\n",
    "                        \"exp\": {\n",
    "                            \"date\": {\n",
    "                                \"origin\": \"now\",\n",
    "                                \"scale\": \"90d\",    # Documents 90 days old get 50% weight\n",
    "                                \"offset\": \"7d\",    # No decay for documents within 7 days\n",
    "                                \"decay\": 0.5\n",
    "                            }\n",
    "                        },\n",
    "                        \"weight\": 2.0  # Increase importance of recency\n",
    "                    }\n",
    "                ],\n",
    "                \"score_mode\": \"sum\",      # Add function scores together\n",
    "                \"boost_mode\": \"multiply\"  # Multiply with query score\n",
    "            }\n",
    "        },\n",
    "        \"knn\": [\n",
    "            {\n",
    "                \"field\": \"title_vector\",\n",
    "                \"query_vector\": query_vector,\n",
    "                \"k\": top_k,\n",
    "                \"num_candidates\": 100,\n",
    "                \"boost\": 2.0  # Title vectors weighted higher\n",
    "            },\n",
    "            {\n",
    "                \"field\": \"content_chunks.vector\",\n",
    "                \"query_vector\": query_vector,\n",
    "                \"k\": top_k,\n",
    "                \"num_candidates\": 100,\n",
    "                \"boost\": 1.0\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Add geo-proximity boost if user location is provided\n",
    "    if user_location and \"lat\" in user_location and \"lon\" in user_location:\n",
    "        query_body[\"query\"][\"function_score\"][\"functions\"].append({\n",
    "            \"gauss\": {\n",
    "                \"geo_location\": {\n",
    "                    \"origin\": {\n",
    "                        \"lat\": user_location[\"lat\"],\n",
    "                        \"lon\": user_location[\"lon\"]\n",
    "                    },\n",
    "                    \"scale\": \"200km\",   # Documents 200km away get 50% weight\n",
    "                    \"offset\": \"50km\",   # No decay within 50km\n",
    "                    \"decay\": 0.5\n",
    "                }\n",
    "            },\n",
    "            \"weight\": 1.5  # Adjust importance of location proximity\n",
    "        })\n",
    "    \n",
    "    # Add temporal range filter if provided\n",
    "    if temporal_filter and \"start\" in temporal_filter and \"end\" in temporal_filter:\n",
    "        query_body[\"query\"][\"function_score\"][\"query\"][\"bool\"][\"filter\"] = [\n",
    "            {\n",
    "                \"range\": {\n",
    "                    \"date\": {\n",
    "                        \"gte\": temporal_filter[\"start\"],\n",
    "                        \"lte\": temporal_filter[\"end\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    # Execute search\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body=query_body,\n",
    "        request_timeout=30\n",
    "    )\n",
    "    \n",
    "    # Return results with full metadata\n",
    "    results = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        results.append({\n",
    "            \"title\": hit[\"_source\"][\"title\"],\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"date\": hit[\"_source\"].get(\"date\"),\n",
    "            \"geo_location\": hit[\"_source\"].get(\"geo_location\"),\n",
    "            \"places\": hit[\"_source\"].get(\"places\", []),\n",
    "            \"authors\": hit[\"_source\"].get(\"authors\", []),\n",
    "            \"content_preview\": hit[\"_source\"][\"content\"][:200] + \"...\"  # First 200 chars\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test cases demonstrating all features\n",
    "print(\"\\n=== Test 1: Basic search ===\")\n",
    "results = hybrid_search(\"banking crisis\")\n",
    "print(f\"Found {len(results)} results:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['title']} (score: {result['score']:.2f})\")\n",
    "\n",
    "print(\"\\n=== Test 2: Fuzzy search (with typo) ===\")\n",
    "results = hybrid_search(\"bankng crsis\")  # Intentional typos\n",
    "print(f\"Found {len(results)} results with typos corrected:\")\n",
    "for i, result in enumerate(results[:3], 1):\n",
    "    print(f\"{i}. {result['title']}\")\n",
    "\n",
    "print(\"\\n=== Test 3: Location-aware search ===\")\n",
    "london_location = {\"lat\": 51.5074, \"lon\": -0.1278}\n",
    "results = hybrid_search(\"banking crisis\", user_location=london_location)\n",
    "print(f\"Found {len(results)} results near London:\")\n",
    "for i, result in enumerate(results[:3], 1):\n",
    "    location = result.get('geo_location', 'N/A')\n",
    "    print(f\"{i}. {result['title']}\")\n",
    "    print(f\"   Location: {location}\")\n",
    "\n",
    "print(\"\\n=== Test 4: Temporal search (1987 documents) ===\")\n",
    "temporal_filter = {\n",
    "    \"start\": \"1987-01-01\",\n",
    "    \"end\": \"1987-12-31\"\n",
    "}\n",
    "results = hybrid_search(\"banking\", temporal_filter=temporal_filter)\n",
    "print(f\"Found {len(results)} results from 1987:\")\n",
    "for i, result in enumerate(results[:3], 1):\n",
    "    print(f\"{i}. {result['title']}\")\n",
    "    print(f\"   Date: {result.get('date', 'N/A')}\")\n",
    "\n",
    "print(\"\\n=== Test 5: Combined spatio-temporal query ===\")\n",
    "# Search for German news near Hamburg in March 1987\n",
    "hamburg_location = {\"lat\": 53.5511, \"lon\": 9.9937}\n",
    "march_1987 = {\n",
    "    \"start\": \"1987-03-01\",\n",
    "    \"end\": \"1987-03-31\"\n",
    "}\n",
    "results = hybrid_search(\n",
    "    query=\"Germany inflation\", \n",
    "    user_location=hamburg_location,\n",
    "    temporal_filter=march_1987,\n",
    "    top_k=5\n",
    ")\n",
    "print(f\"Found {len(results)} German news near Hamburg in March 1987:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['title']}\")\n",
    "    print(f\"   Date: {result.get('date', 'N/A')}\")\n",
    "    print(f\"   Places: {', '.join(result.get('places', []))}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewsRetrival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

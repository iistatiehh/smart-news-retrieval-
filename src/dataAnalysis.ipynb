{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6557549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 SGM files\n",
      "\n",
      "Reading reut2-000.sgm...\n",
      "  Found 1000 documents\n",
      "Reading reut2-001.sgm...\n",
      "  Found 1000 documents\n",
      "Reading reut2-002.sgm...\n",
      "  Found 1000 documents\n",
      "\n",
      "Total documents: 3000\n",
      "================================================================================\n",
      "\n",
      "Sample Documents:\n",
      "\n",
      "[Document 1] ID: 1\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:01:01.79\n",
      "Title: BAHIA COCOA REVIEW\n",
      "Dateline: SALVADOR, Feb 26 -\n",
      "Body: Showers continued throughout the week in\n",
      "the Bahia cocoa zone, alleviating the drought since early\n",
      "January and improving prospects for the coming temp...\n",
      "Places: el-salvador, usa, uruguay\n",
      "Topics: cocoa\n",
      "\n",
      "[Document 2] ID: 2\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:02:20.00\n",
      "Title: STANDARD OIL <SRD> TO FORM FINANCIAL UNIT\n",
      "Dateline: CLEVELAND, Feb 26 -\n",
      "Body: Standard Oil Co and BP North America\n",
      "Inc said they plan to form a venture to manage the money market\n",
      "borrowing and investment activities of both compa...\n",
      "Places: usa\n",
      "\n",
      "[Document 3] ID: 3\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:03:27.51\n",
      "Title: TEXAS COMMERCE BANCSHARES <TCB> FILES PLAN\n",
      "Dateline: HOUSTON, Feb 26 -\n",
      "Body: Texas Commerce Bancshares Inc's Texas\n",
      "Commerce Bank-Houston said it filed an application with the\n",
      "Comptroller of the Currency in an effort to create t...\n",
      "Places: usa\n",
      "\n",
      "[Document 4] ID: 4\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:07:13.72\n",
      "Title: TALKING POINT/BANKAMERICA <BAC> EQUITY OFFER\n",
      "Dateline: LOS ANGELES, Feb 26 -\n",
      "Body: BankAmerica Corp is not under\n",
      "pressure to act quickly on its proposed equity offering and\n",
      "would do well to delay it because of the stock's recent poor...\n",
      "Places: usa, brazil\n",
      "\n",
      "[Document 5] ID: 5\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:10:44.60\n",
      "Title: NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE\n",
      "Dateline: WASHINGTON, Feb 26 -\n",
      "Body: The U.S. Agriculture Department\n",
      "reported the farmer-owned reserve national five-day average\n",
      "price through February 25 as follows (Dlrs/Bu-Sorghum Cwt)...\n",
      "Places: usa\n",
      "Topics: grain, wheat, corn, barley, oat, sorghum\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "--------------------------------------------------------------------------------\n",
      "Documents with body text: 2761/3000 (92.0%)\n",
      "Documents with places: 2714/3000 (90.5%)\n",
      "Documents with topics: 1600/3000 (53.3%)\n",
      "Documents with people: 129/3000 (4.3%)\n",
      "Documents with organizations: 143/3000 (4.8%)\n",
      "Documents with exchanges: 53/3000 (1.8%)\n",
      "Documents with companies: 0/3000 (0.0%)\n",
      "\n",
      "Unique places (106): ['afghanistan', 'algeria', 'argentina', 'australia', 'austria', 'bahrain', 'bangladesh', 'belgium', 'bhutan', 'bolivia', 'brazil', 'canada', 'cayman-islands', 'chile', 'china', 'colombia', 'congo', 'costa-rica', 'cuba', 'czechoslovakia']\n",
      "  ... and 86 more\n",
      "\n",
      "Unique topics (88): ['acq', 'alum', 'barley', 'bop', 'can', 'carcass', 'citruspulp', 'cocoa', 'coffee', 'copper', 'copra-cake', 'corn', 'cornglutenfeed', 'cotton', 'cpi', 'crude', 'dlr', 'earn', 'fishmeal', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'linseed', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'nat-gas', 'nickel', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'plywood', 'potato', 'propane', 'rape-meal', 'rape-oil', 'rapeseed', 'red-bean', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'saudriyal', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-oil', 'sunseed', 'tapioca', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wool', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "def parse_reuters_document(text):\n",
    "    # extract reuters articles from text\n",
    "    pattern = r'<REUTERS.*?</REUTERS>'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "def clean_xml(xml_text):\n",
    "    # remove invalid xml characters\n",
    "    xml_text = re.sub(r'&#\\d+;', '', xml_text)\n",
    "    # remove special characters that might cause issues\n",
    "    xml_text = re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]', '', xml_text)\n",
    "    return xml_text\n",
    "\n",
    "def extract_fields(reuters_xml):\n",
    "    # parse xml and get fields\n",
    "    try:\n",
    "        cleaned = clean_xml(reuters_xml)\n",
    "        root = ET.fromstring(cleaned)\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Parse error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    doc = {\n",
    "        'newid': root.get('NEWID', ''),\n",
    "        'oldid': root.get('OLDID', ''),\n",
    "        'topics_tag': root.get('TOPICS', ''),\n",
    "        'lewissplit': root.get('LEWISSPLIT', ''),\n",
    "        'date': '',\n",
    "        'title': '',\n",
    "        'dateline': '',\n",
    "        'body': '',\n",
    "        'topics': [],\n",
    "        'places': [],\n",
    "        'people': [],\n",
    "        'orgs': [],\n",
    "        'exchanges': [],\n",
    "        'companies': []\n",
    "    }\n",
    "    \n",
    "    # get date\n",
    "    date_elem = root.find('DATE')\n",
    "    if date_elem is not None and date_elem.text:\n",
    "        doc['date'] = date_elem.text.strip()\n",
    "    \n",
    "    # get text content\n",
    "    text_elem = root.find('TEXT')\n",
    "    if text_elem is not None:\n",
    "        title_elem = text_elem.find('TITLE')\n",
    "        if title_elem is not None and title_elem.text:\n",
    "            doc['title'] = title_elem.text.strip()\n",
    "        \n",
    "        dateline_elem = text_elem.find('DATELINE')\n",
    "        if dateline_elem is not None and dateline_elem.text:\n",
    "            doc['dateline'] = dateline_elem.text.strip()\n",
    "        \n",
    "        body_elem = text_elem.find('BODY')\n",
    "        if body_elem is not None and body_elem.text:\n",
    "            doc['body'] = body_elem.text.strip()\n",
    "    \n",
    "    # get all categories with explicit checks\n",
    "    topics_elem = root.find('TOPICS')\n",
    "    if topics_elem is not None:\n",
    "        for d in topics_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['topics'].append(d.text.strip())\n",
    "    \n",
    "    places_elem = root.find('PLACES')\n",
    "    if places_elem is not None:\n",
    "        for d in places_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['places'].append(d.text.strip())\n",
    "    \n",
    "    people_elem = root.find('PEOPLE')\n",
    "    if people_elem is not None:\n",
    "        for d in people_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['people'].append(d.text.strip())\n",
    "    \n",
    "    orgs_elem = root.find('ORGS')\n",
    "    if orgs_elem is not None:\n",
    "        for d in orgs_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['orgs'].append(d.text.strip())\n",
    "    \n",
    "    exchanges_elem = root.find('EXCHANGES')\n",
    "    if exchanges_elem is not None:\n",
    "        for d in exchanges_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['exchanges'].append(d.text.strip())\n",
    "    \n",
    "    companies_elem = root.find('COMPANIES')\n",
    "    if companies_elem is not None:\n",
    "        for d in companies_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['companies'].append(d.text.strip())\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# main execution\n",
    "data_path = r\"C:\\Users\\asus\\Desktop\\NewsIndexing\\data\"\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "# find all sgm files\n",
    "sgm_files = [f for f in os.listdir(data_path) if f.endswith('.sgm')]\n",
    "print(f\"Found {len(sgm_files)} SGM files\\n\")\n",
    "\n",
    "# read each file\n",
    "for filename in sgm_files[:3]:\n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    print(f\"Reading {filename}...\")\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    docs = parse_reuters_document(content)\n",
    "    all_documents.extend(docs)\n",
    "    print(f\"  Found {len(docs)} documents\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(all_documents)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# show sample documents\n",
    "print(\"\\nSample Documents:\")\n",
    "for i, doc_xml in enumerate(all_documents[:5]):\n",
    "    doc = extract_fields(doc_xml)\n",
    "    \n",
    "    if doc is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n[Document {i+1}] ID: {doc['newid']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Date: {doc['date']}\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Dateline: {doc['dateline']}\")\n",
    "    \n",
    "    body_preview = doc['body'][:150] + \"...\" if len(doc['body']) > 150 else doc['body']\n",
    "    print(f\"Body: {body_preview}\")\n",
    "    \n",
    "    if doc['places']:\n",
    "        print(f\"Places: {', '.join(doc['places'])}\")\n",
    "    if doc['topics']:\n",
    "        print(f\"Topics: {', '.join(doc['topics'])}\")\n",
    "    if doc['people']:\n",
    "        print(f\"People: {', '.join(doc['people'])}\")\n",
    "    if doc['orgs']:\n",
    "        print(f\"Organizations: {', '.join(doc['orgs'])}\")\n",
    "    if doc['exchanges']:\n",
    "        print(f\"Exchanges: {', '.join(doc['exchanges'])}\")\n",
    "    if doc['companies']:\n",
    "        print(f\"Companies: {', '.join(doc['companies'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# calculate statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "all_docs = [extract_fields(doc_xml) for doc_xml in all_documents]\n",
    "all_docs = [d for d in all_docs if d is not None]\n",
    "\n",
    "docs_with_topics = sum(1 for d in all_docs if d['topics'])\n",
    "docs_with_places = sum(1 for d in all_docs if d['places'])\n",
    "docs_with_people = sum(1 for d in all_docs if d['people'])\n",
    "docs_with_orgs = sum(1 for d in all_docs if d['orgs'])\n",
    "docs_with_exchanges = sum(1 for d in all_docs if d['exchanges'])\n",
    "docs_with_companies = sum(1 for d in all_docs if d['companies'])\n",
    "docs_with_body = sum(1 for d in all_docs if d['body'])\n",
    "\n",
    "total = len(all_docs)\n",
    "print(f\"Documents with body text: {docs_with_body}/{total} ({100*docs_with_body/total:.1f}%)\")\n",
    "print(f\"Documents with places: {docs_with_places}/{total} ({100*docs_with_places/total:.1f}%)\")\n",
    "print(f\"Documents with topics: {docs_with_topics}/{total} ({100*docs_with_topics/total:.1f}%)\")\n",
    "print(f\"Documents with people: {docs_with_people}/{total} ({100*docs_with_people/total:.1f}%)\")\n",
    "print(f\"Documents with organizations: {docs_with_orgs}/{total} ({100*docs_with_orgs/total:.1f}%)\")\n",
    "print(f\"Documents with exchanges: {docs_with_exchanges}/{total} ({100*docs_with_exchanges/total:.1f}%)\")\n",
    "print(f\"Documents with companies: {docs_with_companies}/{total} ({100*docs_with_companies/total:.1f}%)\")\n",
    "\n",
    "# show unique values\n",
    "all_places = set()\n",
    "all_topics = set()\n",
    "for d in all_docs:\n",
    "    all_places.update(d['places'])\n",
    "    all_topics.update(d['topics'])\n",
    "\n",
    "print(f\"\\nUnique places ({len(all_places)}): {sorted(list(all_places))[:20]}\")\n",
    "if len(all_places) > 20:\n",
    "    print(f\"  ... and {len(all_places) - 20} more\")\n",
    "\n",
    "print(f\"\\nUnique topics ({len(all_topics)}): {sorted(list(all_topics))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee713b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 SGM files\n",
      "\n",
      "Processing reut2-000.sgm... Found 1000 documents\n",
      "Processing reut2-001.sgm... Found 1000 documents\n",
      "Processing reut2-002.sgm... Found 1000 documents\n",
      "Processing reut2-003.sgm... Found 1000 documents\n",
      "Processing reut2-004.sgm... Found 1000 documents\n",
      "Processing reut2-005.sgm... Found 1000 documents\n",
      "Processing reut2-006.sgm... Found 1000 documents\n",
      "Processing reut2-007.sgm... Found 1000 documents\n",
      "Processing reut2-008.sgm... Found 1000 documents\n",
      "Processing reut2-009.sgm... Found 1000 documents\n",
      "Processing reut2-010.sgm... Found 1000 documents\n",
      "Processing reut2-011.sgm... Found 1000 documents\n",
      "Processing reut2-012.sgm... Found 1000 documents\n",
      "Processing reut2-013.sgm... Found 1000 documents\n",
      "Processing reut2-014.sgm... Found 1000 documents\n",
      "Processing reut2-015.sgm... Found 1000 documents\n",
      "Processing reut2-016.sgm... Found 1000 documents\n",
      "Processing reut2-017.sgm... Found 1000 documents\n",
      "Processing reut2-018.sgm... Found 1000 documents\n",
      "Processing reut2-019.sgm... Found 1000 documents\n",
      "Processing reut2-020.sgm... Found 1000 documents\n",
      "Processing reut2-021.sgm... Found 578 documents\n",
      "\n",
      "Successfully parsed 21578 documents\n",
      "\n",
      "Saving to reuters_documents.json...\n",
      "Done! Saved 21578 documents to reuters_documents.json\n",
      "\n",
      "Sample document:\n",
      "{\n",
      "  \"newid\": \"1\",\n",
      "  \"oldid\": \"5544\",\n",
      "  \"topics_tag\": \"YES\",\n",
      "  \"lewissplit\": \"TRAIN\",\n",
      "  \"cgisplit\": \"TRAINING-SET\",\n",
      "  \"date\": \"26-FEB-1987 15:01:01.79\",\n",
      "  \"title\": \"BAHIA COCOA REVIEW\",\n",
      "  \"dateline\": \"SALVADOR, Feb 26 -\",\n",
      "  \"body\": \"Showers continued throughout the week in\\nthe Bahia cocoa zone, alleviating the drought since early\\nJanuary and improving prospects for the coming temporao,\\nalthough normal humidity levels have not been restored,\\nComissaria Smith said in its weekly review.\\n    The dry period means the temporao will be late this year.\\n    Arrivals for the week ended February 22 were 155,221 bags\\nof 60 kilos making a cumulative total for the season of 5.93\\nmln against 5.81 at the same stage last year. Again it seems\\nthat cocoa delivered earlier on consignment was included in the\\narrivals figures.\\n    Comissaria Smith said there is still some doubt as to how\\nmuch old crop cocoa is still available as harvesting has\\npractically come to an end. With total Bahia crop estimates\\naround 6.4 mln bags and sales standing at almost 6.2 mln there\\nare a few hundred thousand bags still in the hands of farmers,\\nmiddlemen, exporters and processors.\\n    There are doubts as to how much of this cocoa would be fit\\nfor export as shippers are now experiencing dificulties in\\nobtaining +Bahia superior+ certificates.\\n    In view of the lower quality over recent weeks farmers have\\nsold a good part of their cocoa held on consignment.\\n    Comissaria Smith said spot bean prices rose to 340 to 350\\ncruzados per arroba of 15 kilos.\\n    Bean shippers were reluctant to offer nearby shipment and\\nonly limited sales were booked for March shipment at 1,750 to\\n1,780 dlrs per tonne to ports to be named.\\n    New crop sales were also light and all to open ports with\\nJune/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\\nunder New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\\nper tonne FOB.\\n    Routine sales of butter were made. March/April sold at\\n4,340, 4,345 and 4,350 dlrs.\\n    April/May butter went at 2.27 times New York May, June/July\\nat 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\\n2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\\n2.27 times New York Dec, Comissaria Smith said.\\n    Destinations were the U.S., Covertible currency areas,\\nUruguay and open ports.\\n    Cake sales were registered at 785 to 995 dlrs for\\nMarch/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\\nNew York Dec for Oct/Dec.\\n    Buyers were the U.S., Argentina, Uruguay and convertible\\ncurrency areas.\\n    Liquor sales were limited with March/April selling at 2,325\\nand 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\\nYork July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\\nSept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\\nsaid.\\n    Total Bahia sales are currently estimated at 6.13 mln bags\\nagainst the 1986/87 crop and 1.06 mln bags against the 1987/88\\ncrop.\\n    Final figures for the period to February 28 are expected to\\nbe published by the Brazilian Cocoa Trade Commission after\\ncarnival which ends midday on February 27.\\n Reuter\",\n",
      "  \"topics\": [\n",
      "    \"cocoa\"\n",
      "  ],\n",
      "  \"places\": [\n",
      "    \"el-salvador\",\n",
      "    \"usa\",\n",
      "    \"uruguay\"\n",
      "  ],\n",
      "  \"people\": [],\n",
      "  \"orgs\": [],\n",
      "  \"exchanges\": [],\n",
      "  \"companies\": []\n",
      "}\n",
      "\n",
      "Dataset Statistics:\n",
      "------------------------------------------------------------\n",
      "Total documents: 21578\n",
      "With body text: 19043 (88.3%)\n",
      "With places: 18798 (87.1%)\n",
      "With topics: 11367 (52.7%)\n",
      "With people: 1156 (5.4%)\n",
      "With organizations: 881 (4.1%)\n",
      "With exchanges: 482 (2.2%)\n",
      "With companies: 0 (0.0%)\n",
      "\n",
      "Unique places: 147\n",
      "Unique topics: 120\n",
      "\n",
      "Sample places: ['afghanistan', 'algeria', 'angola', 'antigua', 'argentina', 'aruba', 'australia', 'austria', 'bahamas', 'bahrain']\n",
      "Sample topics: ['acq', 'alum', 'austdlr', 'barley', 'bfr', 'bop', 'can', 'carcass', 'castor-oil', 'castorseed']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "def parse_reuters_document(text):\n",
    "    # extract reuters articles from text\n",
    "    pattern = r'<REUTERS.*?</REUTERS>'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "def clean_xml(xml_text):\n",
    "    # remove invalid xml characters\n",
    "    xml_text = re.sub(r'&#\\d+;', '', xml_text)\n",
    "    xml_text = re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]', '', xml_text)\n",
    "    return xml_text\n",
    "\n",
    "def extract_fields(reuters_xml):\n",
    "    # parse xml and get fields\n",
    "    try:\n",
    "        cleaned = clean_xml(reuters_xml)\n",
    "        root = ET.fromstring(cleaned)\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Parse error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    doc = {\n",
    "        'newid': root.get('NEWID', ''),\n",
    "        'oldid': root.get('OLDID', ''),\n",
    "        'topics_tag': root.get('TOPICS', ''),\n",
    "        'lewissplit': root.get('LEWISSPLIT', ''),\n",
    "        'cgisplit': root.get('CGISPLIT', ''),\n",
    "        'date': '',\n",
    "        'title': '',\n",
    "        'dateline': '',\n",
    "        'body': '',\n",
    "        'topics': [],\n",
    "        'places': [],\n",
    "        'people': [],\n",
    "        'orgs': [],\n",
    "        'exchanges': [],\n",
    "        'companies': []\n",
    "    }\n",
    "    \n",
    "    # get date\n",
    "    date_elem = root.find('DATE')\n",
    "    if date_elem is not None and date_elem.text:\n",
    "        doc['date'] = date_elem.text.strip()\n",
    "    \n",
    "    # get text content\n",
    "    text_elem = root.find('TEXT')\n",
    "    if text_elem is not None:\n",
    "        title_elem = text_elem.find('TITLE')\n",
    "        if title_elem is not None and title_elem.text:\n",
    "            doc['title'] = title_elem.text.strip()\n",
    "        \n",
    "        dateline_elem = text_elem.find('DATELINE')\n",
    "        if dateline_elem is not None and dateline_elem.text:\n",
    "            doc['dateline'] = dateline_elem.text.strip()\n",
    "        \n",
    "        body_elem = text_elem.find('BODY')\n",
    "        if body_elem is not None and body_elem.text:\n",
    "            doc['body'] = body_elem.text.strip()\n",
    "    \n",
    "    # get all categories\n",
    "    topics_elem = root.find('TOPICS')\n",
    "    if topics_elem is not None:\n",
    "        for d in topics_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['topics'].append(d.text.strip())\n",
    "    \n",
    "    places_elem = root.find('PLACES')\n",
    "    if places_elem is not None:\n",
    "        for d in places_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['places'].append(d.text.strip())\n",
    "    \n",
    "    people_elem = root.find('PEOPLE')\n",
    "    if people_elem is not None:\n",
    "        for d in people_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['people'].append(d.text.strip())\n",
    "    \n",
    "    orgs_elem = root.find('ORGS')\n",
    "    if orgs_elem is not None:\n",
    "        for d in orgs_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['orgs'].append(d.text.strip())\n",
    "    \n",
    "    exchanges_elem = root.find('EXCHANGES')\n",
    "    if exchanges_elem is not None:\n",
    "        for d in exchanges_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['exchanges'].append(d.text.strip())\n",
    "    \n",
    "    companies_elem = root.find('COMPANIES')\n",
    "    if companies_elem is not None:\n",
    "        for d in companies_elem.findall('D'):\n",
    "            if d.text and d.text.strip():\n",
    "                doc['companies'].append(d.text.strip())\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# main execution\n",
    "data_path = r\"C:\\Users\\asus\\Desktop\\NewsIndexing\\data\"\n",
    "output_file = \"reuters_documents.json\"\n",
    "\n",
    "all_documents = []\n",
    "parsed_docs = []\n",
    "\n",
    "# find all sgm files\n",
    "sgm_files = sorted([f for f in os.listdir(data_path) if f.endswith('.sgm')])\n",
    "print(f\"Found {len(sgm_files)} SGM files\\n\")\n",
    "\n",
    "# read each file\n",
    "for filename in sgm_files:\n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    print(f\"Processing {filename}...\", end=\" \")\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    docs = parse_reuters_document(content)\n",
    "    print(f\"Found {len(docs)} documents\")\n",
    "    \n",
    "    # parse each document\n",
    "    for doc_xml in docs:\n",
    "        doc = extract_fields(doc_xml)\n",
    "        if doc is not None:\n",
    "            parsed_docs.append(doc)\n",
    "\n",
    "print(f\"\\nSuccessfully parsed {len(parsed_docs)} documents\")\n",
    "\n",
    "# save to json file\n",
    "print(f\"\\nSaving to {output_file}...\")\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(parsed_docs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Saved {len(parsed_docs)} documents to {output_file}\")\n",
    "\n",
    "# show sample\n",
    "print(\"\\nSample document:\")\n",
    "print(json.dumps(parsed_docs[0], indent=2))\n",
    "\n",
    "# show statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "total = len(parsed_docs)\n",
    "docs_with_body = sum(1 for d in parsed_docs if d['body'])\n",
    "docs_with_places = sum(1 for d in parsed_docs if d['places'])\n",
    "docs_with_topics = sum(1 for d in parsed_docs if d['topics'])\n",
    "docs_with_people = sum(1 for d in parsed_docs if d['people'])\n",
    "docs_with_orgs = sum(1 for d in parsed_docs if d['orgs'])\n",
    "docs_with_exchanges = sum(1 for d in parsed_docs if d['exchanges'])\n",
    "docs_with_companies = sum(1 for d in parsed_docs if d['companies'])\n",
    "\n",
    "print(f\"Total documents: {total}\")\n",
    "print(f\"With body text: {docs_with_body} ({100*docs_with_body/total:.1f}%)\")\n",
    "print(f\"With places: {docs_with_places} ({100*docs_with_places/total:.1f}%)\")\n",
    "print(f\"With topics: {docs_with_topics} ({100*docs_with_topics/total:.1f}%)\")\n",
    "print(f\"With people: {docs_with_people} ({100*docs_with_people/total:.1f}%)\")\n",
    "print(f\"With organizations: {docs_with_orgs} ({100*docs_with_orgs/total:.1f}%)\")\n",
    "print(f\"With exchanges: {docs_with_exchanges} ({100*docs_with_exchanges/total:.1f}%)\")\n",
    "print(f\"With companies: {docs_with_companies} ({100*docs_with_companies/total:.1f}%)\")\n",
    "\n",
    "# show unique values\n",
    "all_places = set()\n",
    "all_topics = set()\n",
    "for d in parsed_docs:\n",
    "    all_places.update(d['places'])\n",
    "    all_topics.update(d['topics'])\n",
    "\n",
    "print(f\"\\nUnique places: {len(all_places)}\")\n",
    "print(f\"Unique topics: {len(all_topics)}\")\n",
    "print(f\"\\nSample places: {sorted(list(all_places))[:10]}\")\n",
    "print(f\"Sample topics: {sorted(list(all_topics))[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

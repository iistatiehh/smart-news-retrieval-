{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97ed9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install elasticsearch sentence-transformers\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca68f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"news_reuters_docs\"\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9202\",\n",
    "    request_timeout=60,\n",
    "    max_retries=3,\n",
    "    retry_on_timeout=True\n",
    ")\n",
    "\n",
    "# Add this to clean up old indices\n",
    "print(\"Cleaning up...\")\n",
    "try:\n",
    "    # Delete old problematic indices\n",
    "    all_indices = es.cat.indices(format='json')\n",
    "    for idx in all_indices:\n",
    "        if idx['health'] == 'red' or idx['status'] == 'close':\n",
    "            print(f\"Deleting problematic index: {idx['index']}\")\n",
    "            es.indices.delete(index=idx['index'], ignore=[400, 404])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a004311",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping ={\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"max_ngram_diff\": 5\n",
    "    },\n",
    "    \"analysis\": {\n",
    "      \"char_filter\": {\n",
    "        \"html_strip\": { \"type\": \"html_strip\" }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"length_filter\": { \"type\": \"length\", \"min\": 3 }\n",
    "      },\n",
    "      \"tokenizer\": {\n",
    "        \"autocomplete_infix_tokenizer\": {\n",
    "          \"type\": \"ngram\",\n",
    "          \"min_gram\": 3,\n",
    "          \"max_gram\": 8,\n",
    "          \"token_chars\": [\"letter\", \"digit\"]\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"autocomplete_infix\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"autocomplete_infix_tokenizer\",\n",
    "          \"filter\": [\"lowercase\"]\n",
    "        },\n",
    "        \"autocomplete_infix_search\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"lowercase\"\n",
    "        },\n",
    "        \"content_analyzer\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"char_filter\": [\"html_strip\"],\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\", \"stop\", \"length_filter\", \"porter_stem\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"autocomplete_infix\",\n",
    "        \"search_analyzer\": \"autocomplete_infix_search\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"title_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 384,\n",
    "        \"index\": True,\n",
    "        \"similarity\": \"cosine\",\n",
    "        \"index_options\": { \"type\": \"hnsw\", \"m\": 16, \"ef_construction\": 100 }\n",
    "      },\n",
    "      \"content_chunks\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"text\": { \"type\": \"text\" },\n",
    "          \"vector\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 384,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"cosine\",\n",
    "            \"index_options\": { \"type\": \"hnsw\", \"m\": 16, \"ef_construction\": 100 }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"content\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"content_analyzer\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"authors\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"first_name\": { \"type\": \"text\" },\n",
    "          \"last_name\": { \"type\": \"text\" },\n",
    "          \"email\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"date\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"strict_date_optional_time||yyyy-MM-dd'T'HH:mm:ss||epoch_millis\"\n",
    "      },\n",
    "      \"dateline\": { \"type\": \"text\" },\n",
    "      \"geo_location\": { \"type\": \"geo_point\" },\n",
    "      \"temporalExpressions\": { \"type\": \"keyword\" },\n",
    "      \"georeferences\": { \"type\": \"keyword\" },\n",
    "      \"places\": { \"type\": \"keyword\" },\n",
    "      \"geopoints\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"place\": { \"type\": \"keyword\" },\n",
    "          \"location\": { \"type\": \"geo_point\" }\n",
    "        }\n",
    "      },\n",
    "      \"topics\": { \"type\": \"keyword\" },\n",
    "      \"people\": { \"type\": \"keyword\" },\n",
    "      \"orgs\": { \"type\": \"keyword\" },\n",
    "      \"exchanges\": { \"type\": \"keyword\" },\n",
    "      \"companies\": { \"type\": \"keyword\" }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e8479b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(es.ping())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07393056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for index to be ready...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'cluster_name': 'elasticsearch', 'status': 'yellow', 'timed_out': False, 'number_of_nodes': 1, 'number_of_data_nodes': 1, 'active_primary_shards': 45, 'active_shards': 45, 'relocating_shards': 0, 'initializing_shards': 0, 'unassigned_shards': 2, 'unassigned_primary_shards': 0, 'delayed_unassigned_shards': 0, 'number_of_pending_tasks': 0, 'number_of_in_flight_fetch': 0, 'task_max_waiting_in_queue_millis': 0, 'active_shards_percent_as_number': 95.74468085106383})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, body=mapping)\n",
    "\n",
    "# Add this - wait for index to be ready\n",
    "print(\"Waiting for index to be ready...\")\n",
    "time.sleep(5)\n",
    "es.cluster.health(wait_for_status='yellow', timeout='30s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6e87fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 21578 documents\n",
      "Processing batch 1 (0 to 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_33044\\721800356.py:103: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  success, failed = helpers.bulk(es, actions, raise_on_error=False, request_timeout=120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 2 (500 to 1000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 3 (1000 to 1500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 4 (1500 to 2000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 5 (2000 to 2500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 6 (2500 to 3000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 7 (3000 to 3500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 8 (3500 to 4000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 9 (4000 to 4500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 10 (4500 to 5000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 11 (5000 to 5500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 12 (5500 to 6000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 13 (6000 to 6500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 14 (6500 to 7000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 15 (7000 to 7500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 16 (7500 to 8000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 17 (8000 to 8500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 18 (8500 to 9000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 19 (9000 to 9500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 20 (9500 to 10000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 21 (10000 to 10500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 22 (10500 to 11000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 23 (11000 to 11500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 24 (11500 to 12000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 25 (12000 to 12500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 26 (12500 to 13000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 27 (13000 to 13500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 28 (13500 to 14000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 29 (14000 to 14500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 30 (14500 to 15000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 31 (15000 to 15500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 32 (15500 to 16000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 33 (16000 to 16500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 34 (16500 to 17000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 35 (17000 to 17500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 36 (17500 to 18000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 37 (18000 to 18500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 38 (18500 to 19000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 39 (19000 to 19500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 40 (19500 to 20000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 41 (20000 to 20500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 42 (20500 to 21000)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 43 (21000 to 21500)\n",
      "  Indexed 500 documents (Failed: 0)\n",
      "Processing batch 44 (21500 to 21578)\n",
      "  Indexed 78 documents (Failed: 0)\n",
      "\n",
      "Total indexed: 21578 documents successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 1, 'failed': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, time, re\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "file_path = r\"C:\\Users\\asus\\Desktop\\NewsRetrival\\smart-news-retrieval-\\output\\reuters_full.json\"\n",
    "\n",
    "def chunk_text(text, max_len=500):\n",
    "    \"\"\"Split text into chunks of max_len characters.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sent in sentences:\n",
    "        if len(current_chunk) + len(sent) <= max_len:\n",
    "            current_chunk += \" \" + sent if current_chunk else sent\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sent\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "print(\"Loading documents...\")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "batch_size = 500\n",
    "total_indexed = 0\n",
    "\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch = documents[i:i + batch_size]\n",
    "    actions = []\n",
    "\n",
    "    print(f\"Processing batch {i//batch_size + 1} ({i} to {min(i+batch_size, len(documents))})\")\n",
    "\n",
    "    for doc in batch:\n",
    "        title = doc.get(\"title\", \"\")\n",
    "        content = doc.get(\"content\", \"\")\n",
    "        date = doc.get(\"date\")\n",
    "        dateline = doc.get(\"dateline\", \"\")\n",
    "        places = doc.get(\"places\", [])\n",
    "        temporal = doc.get(\"temporalExpressions\", [])\n",
    "        georefs = doc.get(\"georeferences\", [])\n",
    "\n",
    "        # Fix authors extraction - properly handle nested objects\n",
    "        authors = []\n",
    "        for author in doc.get(\"authors\", []):\n",
    "            if author:\n",
    "                authors.append({\n",
    "                    \"first_name\": author.get(\"first_name\", \"\"),\n",
    "                    \"last_name\": author.get(\"last_name\", \"\"),\n",
    "                    \"email\": author.get(\"email\", \"\")\n",
    "                })\n",
    "\n",
    "        # Fix geopoints extraction - properly handle nested objects\n",
    "        geopoints = []\n",
    "        for g in doc.get(\"geopoints\", []):\n",
    "            if g and g.get(\"location\"):\n",
    "                loc = g[\"location\"]\n",
    "                lat = loc.get(\"lat\")\n",
    "                lon = loc.get(\"lon\")\n",
    "                place = g.get(\"place\", \"\")\n",
    "                \n",
    "                if lat is not None and lon is not None:\n",
    "                    geopoints.append({\n",
    "                        \"place\": place,\n",
    "                        \"location\": {\"lat\": lat, \"lon\": lon}\n",
    "                    })\n",
    "        \n",
    "        geo_location = None\n",
    "        if geopoints:\n",
    "            geo_location = geopoints[0][\"location\"]\n",
    "        \n",
    "        # title vector\n",
    "        title_vector = model.encode(title).tolist()\n",
    "\n",
    "        # chunk content and encode each chunk\n",
    "        content_chunks = chunk_text(content, max_len=500)\n",
    "        content_chunks_encoded = [\n",
    "            {\"text\": chunk, \"vector\": model.encode(chunk).tolist()} for chunk in content_chunks\n",
    "        ]\n",
    "\n",
    "        es_doc = {\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_source\": {\n",
    "                \"title\": title,\n",
    "                \"title_vector\": title_vector,\n",
    "                \"content\": content,\n",
    "                \"content_chunks\": content_chunks_encoded,\n",
    "                \"authors\": authors,\n",
    "                \"date\": date,\n",
    "                \"dateline\": dateline,\n",
    "                \"places\": places,\n",
    "                \"temporalExpressions\": temporal,\n",
    "                \"georeferences\": georefs,\n",
    "                \"geopoints\": geopoints,\n",
    "                \"geo_location\": geo_location\n",
    "            }\n",
    "        }\n",
    "        actions.append(es_doc)\n",
    "\n",
    "    try:\n",
    "        success, failed = helpers.bulk(es, actions, raise_on_error=False, request_timeout=120)\n",
    "        total_indexed += success\n",
    "        print(f\"  Indexed {success} documents (Failed: {len(failed)})\")\n",
    "        if failed:\n",
    "            print(f\"  First error: {failed[0]}\")\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error in batch: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nTotal indexed: {total_indexed} documents successfully.\")\n",
    "es.indices.refresh(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing search with HNSW...\n",
      "Found 10 results:\n",
      "1. BRAZIL'S DEBT CRISIS BECOMING POLITICAL CRISIS\n",
      "2. EX-ARCO <ARC> CHIEF SEES ENERGY CRISIS BY 1990\n",
      "3. EC FARM CRISIS LIKELY TO GO TO END-JUNE SUMMIT\n",
      "4. JAPAN SEES NEED TO ACT QUICKLY ON TRADE CRISIS\n",
      "5. G-24 SAYS DEBT CRISIS ENTERING NEW PHASE\n",
      "6. BANKING CENTER <TBCX> 1ST QTR NET\n",
      "7. BANKING CENTER <TBCX.O> 3RD QTR NET\n",
      "8. SIGNET BANKING CORP <SBK> 3RD QTR NET\n",
      "9. THE BANKING CENTER <TBCX.O> 3RD QTR NET\n",
      "10. CASH CRISIS HITS UGANDAN COFFEE BOARD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_33044\\822203351.py:14: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  response = es.search(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def hybrid_search(query, top_k=10):\n",
    "    query_vector = model.encode(query).tolist()\n",
    "    \n",
    "    # lexical search on title and content\n",
    "    lexical_query = {\n",
    "        \"multi_match\": {\n",
    "            \"query\": query,\n",
    "            \"fields\": [\"title^3\", \"content\"],\n",
    "            \"type\": \"best_fields\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Use kNN for faster HNSW vector search\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": top_k,\n",
    "            \"query\": lexical_query,\n",
    "            \"knn\": [\n",
    "                {\n",
    "                    \"field\": \"title_vector\",\n",
    "                    \"query_vector\": query_vector,\n",
    "                    \"k\": top_k,\n",
    "                    \"num_candidates\": 100\n",
    "                },\n",
    "                {\n",
    "                    \"field\": \"content_chunks.vector\",  # Change from content_vector\n",
    "                    \"query_vector\": query_vector,\n",
    "                    \"k\": top_k,\n",
    "                    \"num_candidates\": 100\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        request_timeout=30\n",
    "    )\n",
    "    \n",
    "    return [hit[\"_source\"][\"title\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "\n",
    "# test\n",
    "print(\"\\nTesting search with HNSW...\")\n",
    "results = hybrid_search(\"banking crisis\")\n",
    "print(f\"Found {len(results)} results:\")\n",
    "for i, title in enumerate(results, 1):\n",
    "    print(f\"{i}. {title}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewsRetrival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
